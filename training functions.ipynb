import torch
import torch.nn as nn
import pandas as pd
import numpy as np
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
# Âä†ËΩΩÊï∞ÊçÆ
data = pd.read_csv("../autodl-tmp/dataset_pinn.csv")
x_raw = data.iloc[:, :-1].values
y_raw = data.iloc[:, -1].values.reshape(-1, 1)

x_tensor = torch.tensor(x_raw, dtype=torch.float32)
y_tensor = torch.tensor(y_raw, dtype=torch.float32)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
x_tensor = x_tensor.to(device)
y_tensor = y_tensor.to(device)

#ÊµãËØïÈõÜ‰∏∫8~60sÁöÑÊâÄÊúâÁÇπ
test_df = data[(data['t'] >= 8) & (data['t'] <= 60)]
X_test = torch.tensor(test_df[['x', 'y', 't']].values, dtype=torch.float32)
h_test = torch.tensor(test_df[['h']].values, dtype=torch.float32)
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
class PINN_MC(nn.Module):
    def __init__(self, layers, dropout_p=0.1):
        super(PINN_MC, self).__init__()
        self.activation = nn.Tanh()
        layer_list = []

        for i in range(len(layers) - 1):
            layer_list.append(nn.Linear(layers[i], layers[i + 1]))
            if i != len(layers) - 2:
                layer_list.append(nn.Dropout(p=dropout_p))  # ÊèíÂÖ• Dropout
                layer_list.append(self.activation)

        self.model = nn.Sequential(*layer_list)

    def forward(self, x):
        return self.model(x)
import torch
import torch.nn.functional as F
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
# ========== 1. ÁΩëÁªúÁªìÊûÑ ==========
class PINN_MC(nn.Module):
    def __init__(self, layers, dropout_p=0.3):
        super(PINN_MC, self).__init__()
        self.activation = nn.Tanh()
        layer_list = []
        for i in range(len(layers) - 1):
            layer_list.append(nn.Linear(layers[i], layers[i + 1]))
            if i != len(layers) - 2:
                layer_list.append(nn.Dropout(p=dropout_p))
                layer_list.append(self.activation)
        self.model = nn.Sequential(*layer_list)

    def forward(self, x):
        return self.model(x)


# ========== 2. ÊâπÊ¨° Dropout MC Êé®Êñ≠==========
def batched_mc_dropout_predict(model, X, mc_times=20, batch_size=512):
    model.train()  # ËÆ© Dropout ÁîüÊïà
    N, d = X.shape
    # [mc_times, N, d]
    X_rep = X.unsqueeze(0).repeat(mc_times, 1, 1)
    # [mc_times * N, d]
    X_rep = X_rep.view(mc_times * N, d)

    # ‰∏ÄÊ¨°ÂâçÂêëÔºöÂØπ mc_times*N ‰∏™Ê†∑Êú¨ÂÅöÈ¢ÑÊµã
    preds = model(X_rep)                  # [mc_times * N, 1]
    preds = preds.view(mc_times, N, -1)   # [mc_times, N, 1]

    mean = preds.mean(dim=0)              # [N, 1]
    var = preds.var(dim=0)                # [N, 1]
    return mean, var


# ========== 3. ‰º†ÁªüÊâπÊ¨°MSE==========
def batched_loss(dataset_X, dataset_Y, model, loss_fn, batch_size=512):
    pred = model(dataset_X)
    loss = loss_fn(pred, dataset_Y)
    return loss


# ========== 4. PDE/Neumann ÊÆãÂ∑Æ ==========
def get_diffusivity(X_f):
    x = X_f[:, 0]
    D = torch.where(
        (x >= 4.0) & (x <= 6.0),
        torch.tensor(0.6, device=x.device, dtype=x.dtype),
        torch.tensor(0.5, device=x.device, dtype=x.dtype)
    )
    return D.view(-1, 1)


def compute_pde_residual(model, X_f):
    X_f.requires_grad_(True)
    h = model(X_f)

    grad_h = torch.autograd.grad(
        h, X_f,
        grad_outputs=torch.ones_like(h),
        retain_graph=True,
        create_graph=True
    )[0]

    h_x = grad_h[:, 0:1]
    h_y = grad_h[:, 1:2]
    h_t = grad_h[:, 2:3]

    h_xx = torch.autograd.grad(
        h_x, X_f,
        grad_outputs=torch.ones_like(h_x),
        retain_graph=True,
        create_graph=True
    )[0][:, 0:1]

    h_yy = torch.autograd.grad(
        h_y, X_f,
        grad_outputs=torch.ones_like(h_y),
        retain_graph=True,
        create_graph=True
    )[0][:, 1:2]

    D = get_diffusivity(X_f)
    residual = h_t - D * (h_xx + h_yy)
    return residual

#neu
def build_inset_neumann_points(X_neu, delta=0.1, Lx=10.0, Ly=6.0):
    """
    Â∞ÜÊó†ÊµÅËæπÁïåÁÇπ X_neu=[x,y,t] Ê≤øÊ≥ïÂêëÂêëÂüüÂÜÖÂπ≥Áßª deltaÔºåÂæóÂà∞ inset ÁÇπ X_in„ÄÇ
    ËæπÁïåÔºöÂ∑¶ x=0„ÄÅÂè≥ x=Lx„ÄÅÂ∫ï y=0(ÈùûÊñ≠Â±ÇÊÆµ)ÔºõÈ°∂ËæπÊòØ DirichletÔºå‰∏çÂú®Ê≠§Â§ÑÁêÜ„ÄÇ
    """
    assert X_neu.dim() == 2 and X_neu.size(1) == 3, f"X_neu shape should be (N,3), got {tuple(X_neu.shape)}"

    X_in = X_neu.detach().clone()  # ‰∏é X_neu ÂêåËÆæÂ§á/Âêådtype
    # Áî® 1D ÂêëÈáè‰æø‰∫éÊåâË°åÊé©Á†Å
    x = X_in[:, 0]
    y = X_in[:, 1]

    # Ë°åÁ∫ßÂ∏ÉÂ∞îÊé©Á†Å (N,)
    left_row  = (x < 1e-5)
    right_row = (x > (Lx - 1e-5))
    bottom_no_fault_row = (y < 1e-5) & ((x < 4.0) | (x > 6.0))

    # Ê≤øÊ≥ïÂêëÂêëÂÜÖÁßªÔºöÂ∑¶(+x)„ÄÅÂè≥(-x)„ÄÅÂ∫ï(+y)
    if left_row.any():
        X_in[left_row, 0] = torch.clamp(x[left_row] + delta, 0.0, Lx)
    if right_row.any():
        X_in[right_row, 0] = torch.clamp(x[right_row] - delta, 0.0, Lx)
    if bottom_no_fault_row.any():
        X_in[bottom_no_fault_row, 1] = torch.clamp(y[bottom_no_fault_row] + delta, 0.0, Ly)

    return X_in


def compute_neumann_hard_loss(model, X_neu, delta=0.1, Lx=10.0, Ly=6.0, normalize_by_delta=False):
    """
    ‰ªÖ‰ΩøÁî®Á°¨Á∫¶ÊùüÔºöh(bdry) ‚âà h(inset)„ÄÇ
    ËøîÂõûÊ†áÈáèÊçüÂ§±ÔºåÂèØÁõ¥Êé•‰πò Œª_neu„ÄÇ
    normalize_by_delta=True Êó∂Èô§‰ª• delta^2„ÄÇ
    """
    # Á©∫Êâπ‰øùÊä§ÔºàÊúâÊó∂ Neumann Ê†∑Êú¨ÂèØËÉΩ‰∏∫ 0Ôºâ
    if X_neu.numel() == 0:
        return torch.zeros([], device=next(model.parameters()).device)

    # inset ÁÇπÔºàÂêåËÆæÂ§á„ÄÅÂêådtypeÔºâ
    X_in = build_inset_neumann_points(X_neu, delta=delta, Lx=Lx, Ly=Ly)

    h_b = model(X_neu)  # ËæπÁïåÂéü‰ΩçÈ¢ÑÊµã
    h_i = model(X_in)   # ÂêëÂÜÖÂπ≥Áßª‰ΩçÁΩÆÁöÑÈ¢ÑÊµã

    loss = F.mse_loss(h_b, h_i)
    if normalize_by_delta:
        loss = loss / (delta ** 2)
    return loss
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import time
import numpy as np
import random

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = 15


# ========== ÊÄªÊçüÂ§±ÂáΩÊï∞Ôºö‰ªÖ UQ ÁÇπÂä† uncertainty Ê≠£Âàô ==========
def total_loss_hybrid_batched_mchu(model,
                                   X_ic, h_ic,
                                   X_bc,
                                   X_f,
                                   X_neu,
                                   X_data=None, h_data=None,
                                   X_uq=None,
                                   Œª_ic=1.0, Œª_bc=1.0, Œª_pde=1, Œª_neu=0.1, Œª_data=10,
                                   Œª_unc=1.0,
                                   mc_times=20,
                                   batch_size=512):
    mse = nn.MSELoss(reduction='mean')
    device = X_f.device

    # 1Ô∏è‚É£ IC ÂàùÂßãÊù°‰ª∂È°π
    h0 = torch.ones_like(h_ic)
    loss_ic = batched_loss(X_ic, h0, model, mse, batch_size)

    # 2Ô∏è‚É£ Dirichlet Êù°‰ª∂È°π
    y_bc = X_bc[:, 1]
    h_bc_labels = torch.where(
        torch.isclose(y_bc, torch.tensor(6.0, device=device)),
        torch.tensor(10.0, device=device),
        torch.tensor(0.0, device=device)
    ).reshape(-1, 1)
    loss_bc = batched_loss(X_bc, h_bc_labels, model, mse, batch_size)

    # 3Ô∏è‚É£ PDE ÊÆãÂ∑ÆÈ°π
    residual_pde = compute_pde_residual(model, X_f)   # [N_f, 1]
    loss_pde = (residual_pde ** 2).mean()

    # 4Ô∏è‚É£ Neumann
    loss_neu = compute_neumann_hard_loss(model, X_neu, delta=0.1, Lx=10.0, Ly=6.0, normalize_by_delta=False)
    
    # 5Ô∏è‚É£ Êï∞ÊçÆÁõëÁù£È°π
    loss_data = torch.tensor(0.0, device=device)
    if X_data is not None and h_data is not None:
        loss_data = batched_loss(X_data, h_data, model, mse, batch_size)

    # 6Ô∏è‚É£ ÊòæÂºè‰∏çÁ°ÆÂÆöÊÄßÊ≠£ÂàôÈ°πÔºà‰ªÖ UQ ÁÇπÔºâ
    unc_loss = torch.tensor(0.0, device=device)
    if X_uq is not None and X_uq.shape[0] > 0:
        # ‰ΩøÁî®‰ºòÂåñÂêéÁöÑ batched_mc_dropout_predictÔºà‰∏ÄÊ¨°Â§ß batchÔºâ
        _, uq_var = batched_mc_dropout_predict(model, X_uq, mc_times, batch_size)
        unc_loss = uq_var.mean()

    total = (
        Œª_ic * loss_ic +
        Œª_bc * loss_bc +
        Œª_pde * loss_pde +
        Œª_neu * loss_neu +
        Œª_data * loss_data +
        Œª_unc * unc_loss
    )
    return total, loss_ic, loss_bc, loss_pde, loss_neu, loss_data, unc_loss


# ==========ËÆ≠ÁªÉ‰∏ªÂæ™ÁéØ ==========
def train_mchu_pinn_batched(model,
                            X_ic, h_ic,
                            X_bc,
                            X_f,
                            X_neu,
                            X_data=None, h_data=None,
                            X_uq=None,
                            epochs=10000,
                            lr=1e-3,
                            batch_size=512,
                            mc_times=20,
                            Œª_unc=1.0,
                            save_path="pinn_mchu.pt",
                            plot_path="loss_curve_mchu.png"):
    device = torch.device("cuda")
    model.to(device).train()
    X_ic, h_ic = X_ic.to(device), h_ic.to(device)
    X_bc = X_bc.to(device)
    X_f = X_f.to(device)
    X_neu = X_neu.to(device)
    if X_data is not None:
        X_data, h_data = X_data.to(device), h_data.to(device)
    if X_uq is not None:
        X_uq = X_uq.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    min_total_loss = float("inf")
    best_epoch = -1
    best_losses = None
    loss_history = []

    print("üöÄ ÂêØÂä® MCHU-PINN ÂàÜÊâπËÆ≠ÁªÉ ...")
    start_time = time.time()

    for epoch in range(1, epochs + 1):
        optimizer.zero_grad()
        total, loss_ic, loss_bc, loss_pde, loss_neu, loss_data, unc_loss = total_loss_hybrid_batched_mchu(
            model,
            X_ic, h_ic,
            X_bc,
            X_f,
            X_neu,
            X_data, h_data,
            X_uq,
            Œª_ic=1.0, Œª_bc=1.0, Œª_pde=1, Œª_neu=0.1, Œª_data=10,
            Œª_unc=Œª_unc,
            mc_times=mc_times,
            batch_size=batch_size
        )
        total.backward()
        optimizer.step()
        loss_history.append(total.item())

        if epoch % 100 == 0:
            print(f"[Epoch {epoch:4d}] "
                  f"Total: {total.item():.4e} | "
                  f"IC: {loss_ic:.2e} | BC: {loss_bc:.2e} | "
                  f"PDE: {loss_pde:.2e} | Neu: {loss_neu:.2e} | "
                  f"Data: {loss_data:.2e} | Unc: {unc_loss:.2e}")

        if total.item() < min_total_loss:
            min_total_loss = total.item()
            best_epoch = epoch
            best_losses = (loss_ic, loss_bc, loss_pde, loss_neu, loss_data, unc_loss)
            torch.save(model.state_dict(), save_path)

    duration = time.time() - start_time
    print(f"\nüèÅ ËÆ≠ÁªÉÂÆåÊàêÔºÅËÄóÊó∂ {duration:.1f} Áßí„ÄÇÊúÄ‰Ω≥Ê®°ÂûãÂ∑≤‰øùÂ≠òÔºö{save_path}")
    print(f"‚úÖ ÊúÄ‰Ω≥ÊÄªÊçüÂ§±Ôºö{min_total_loss:.4e}ÔºàÁ¨¨ {best_epoch} ËΩÆÔºâ")
    if best_losses:
        print(f"    IC:   {best_losses[0]:.2e}")
        print(f"    BC:   {best_losses[1]:.2e}")
        print(f"    PDE:  {best_losses[2]:.2e}")
        print(f"    Neu:  {best_losses[3]:.2e}")
        print(f"    Data: {best_losses[4]:.2e}")
        print(f"    Unc:  {best_losses[5]:.2e}")

    # üé® ÁªòÂõæ
    plt.figure(figsize=(8, 5))
    plt.plot(loss_history, label="Total Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title("MCHU-PINN Training Loss Curve")
    plt.yscale("log")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(plot_path)
    plt.show()
    print(f"üìâ ÊçüÂ§±Êõ≤Á∫øÂ∑≤‰øùÂ≠òÔºö{plot_path}")


# ========== 7. ËÆ≠ÁªÉË∞ÉÁî® ==========
model_mchu = PINN_MC([3, 128, 128, 128, 1], dropout_p=0.3)
train_mchu_pinn_batched(
    model_mchu,
    X_ic, h_ic,
    X_bc,
    X_f,
    X_neu,
    X_data, h_data,
    X_uq,
    epochs=20000,
    lr=1e-3,
    batch_size=8192,
    mc_times=20,
    Œª_unc=1.0,
    save_path="pinn_mchu.pt(8000,20)",
    plot_path="loss_curve_mchu.png"
)
